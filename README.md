````{"id":"23984","variant":"standard","title":"README - Light Agent Bot"}
# Light Agent Bot

Asistente de IA ligero orientado a **agendamiento de citas m√©dicas** (solicitar, consultar y cancelar), con verificaci√≥n de identidad y uso estricto de herramientas.  
Este proyecto demuestra c√≥mo **orquestar un LLM (vLLM)** con reglas de negocio definidas, *system prompt* bien estructurado y control de flujo para evitar comportamientos no deseados.

> Repo: [https://github.com/MazueraAlvaro/light-agent-bot](https://github.com/MazueraAlvaro/light-agent-bot)

---

## üöÄ Caracter√≠sticas

- **Agente m√©dico** con tres funciones principales: **asignar**, **consultar** y **cancelar** citas.  
- **Verificaci√≥n de identidad** antes de cada acci√≥n.  
- **Uso de herramientas controladas** (`listAvailability`, `createAppointment`, `getAppointment`, `cancelAppointment`).  
- Cumplimiento del principio de **m√≠nimo necesario (PHI)** y **no divulgaci√≥n de datos internos**.  
- Integraci√≥n con **vLLM** compatible con la API de OpenAI (`/v1/chat/completions`).  

---

## üß± Arquitectura General

```
[Cliente / Frontend]  ‚Üí  [Light Agent Bot API]  ‚Üí  [vLLM: Qwen3-4B-Instruct]
                                 ‚îÇ
                                 ‚îú‚îÄ‚îÄ verifyPatient / listAvailability / createAppointment / cancelAppointment
                                 ‚îî‚îÄ‚îÄ Servicios / DB de negocio
```

- El **modelo LLM** se usa para el razonamiento y comprensi√≥n.  
- La **l√≥gica de negocio** (verificaci√≥n, validaci√≥n, pol√≠ticas) se mantiene en el backend.  

---

## üì¶ Requisitos

- GPU NVIDIA con soporte CUDA.  
- Docker + nvidia-container-toolkit configurado.  
- Token de Hugging Face v√°lido (`HUGGING_FACE_HUB_TOKEN`).  
- Node.js 18+ o entorno compatible.  

---

## ‚öôÔ∏è Variables de entorno (.env)

Ejemplo de configuraci√≥n m√≠nima:

```bash
PORT=3000
NODE_ENV=development

# vLLM endpoint
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_MODEL=Qwen/Qwen3-4B-Instruct-2507

# APIs de negocio
APPOINTMENTS_API_URL=http://localhost:4000
APPOINTMENTS_API_KEY=change-me
```

---

## üèÉ‚Äç‚ôÇÔ∏è Ejecuci√≥n r√°pida

### 1. Clonar e instalar

```bash
git clone https://github.com/MazueraAlvaro/light-agent-bot.git
cd light-agent-bot
npm install
```

---

### 2. Ejecutar **vLLM con Docker**

> Aseg√∫rate de tener instalado `nvidia-container-toolkit`.  
> Sustituye `hf_<TOKEN>` por tu token de Hugging Face.

```bash
docker run --cpus="2" --runtime nvidia --gpus all \
  --name vllm_container_qwen_opt \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  --env "HUGGING_FACE_HUB_TOKEN=hf_<TOKEN>" \
  -p 8000:8000 \
  --shm-size=2g \
  --ipc=host \
  vllm/vllm-openai:latest \
  --model Qwen/Qwen3-4B-Instruct-2507 \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --max_model_len 43472 \
  --max-num-seqs 384
```

üîπ **Sugerencias:**
- Ajusta `--max-num-seqs` si tu GPU tiene menos VRAM.  
- Usa `--max_model_len` seg√∫n el tama√±o m√°ximo de contexto esperado.  
- Verifica el uso de cache en los logs (`GPU KV cache usage`).  

---

### 3. Probar el endpoint del modelo

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-4B-Instruct-2507",
    "messages": [{"role": "user", "content": "What is the capital of France?"}]
  }'
```

Deber√≠as recibir una respuesta que contenga `"Paris"`.

---

### 4. Iniciar la aplicaci√≥n del agente

```bash
npm run start:dev
# o para producci√≥n
npm run build && npm run start:prod
```

---

## üß† Comportamiento del agente

- Verifica identidad antes de cualquier acci√≥n.  
- Solo ofrece citas **existentes** mediante `listAvailability`.  
- Utiliza **selecciones numeradas** en lugar de IDs visibles.  
- No genera informaci√≥n m√©dica ni respuestas fuera de su dominio.  
- Mantiene **alta precisi√≥n y control de contexto** a trav√©s del prompt del sistema.  

---

## üî¨ Pruebas de carga (k6)

Ejemplo de prueba:

```js
import http from 'k6/http';
import { check } from 'k6';

export const options = { vus: 50, duration: '30s' };

export default function () {
  const res = http.post('http://localhost:8000/v1/chat/completions', JSON.stringify({
    model: 'Qwen/Qwen3-4B-Instruct-2507',
    messages: [{ role: 'user', content: 'What is the capital of France?' }],
  }), { headers: { 'Content-Type': 'application/json' } });

  check(res, {
    'status 200': r => r.status === 200,
    'mentions Paris': r => (r.json()?.choices?.[0]?.message?.content || '').toLowerCase().includes('paris')
  });
}
```

---

## üß© Problemas comunes

| Problema | Soluci√≥n |
|-----------|-----------|
| **OOM al arrancar vLLM** | Reduce `--max-num-seqs` o `--max_model_len`. |
| **Latencia alta (p95)** | Ajusta `max_tokens` en requests o reduce `--max-num-seqs`. |
| **Respuestas inventadas** | Revisa el *system prompt* y las reglas de uso de herramientas. |

---

## üìú Licencia

MIT

---

## ü§ù Contribuciones

1. Crea un *issue* con la descripci√≥n del cambio.  
2. Haz un fork y una rama con tu mejora.  
3. Abre un *Pull Request* con detalle de pruebas y comportamiento esperado.  

---
````
